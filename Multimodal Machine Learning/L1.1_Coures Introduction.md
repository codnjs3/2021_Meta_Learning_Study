## [📔 L1_Course_Introduction]

#### Multi-Modal vs. Multi-Medium
> - **Modality**
> : (이미 일어났거나 경험된) 특정 타입의 정보 or 정보가 저장되는 representation format
> <br>(e.g. Natural language, Visual, Auditory, Haptics/touch, Smell, taste, 
> self-motion, Physiological signals, etc)
> <br> - **Medium**
> : 정보 저장 or 통신을 위한 수단/도구

#### 4 eras of Multimodal research
> 1. The **"behavioral"** era (1970s ~ late 1980s): McGurk effect 발견...!(하나의 modality만으로는 쪼꼼 힘들겠다~)
> 2. The **"computational"** era (late 1980s ~ 2000): Audio-Visual Speech Recognition, Multimodal/Multisensory interfaces, Multimedia Computing
> 3. The **"interaction"** era (2000 ~ 2010): Modeling Human Multimodal Interaction(e.g. Siri)
> 4. The **"deep learning"** era (2010s ~): Representation learning(New large-scale multimodal datasets, improvement of computing power, high-level visual features, "dimensional" linguistic features)

#### Core Technical Challenges
> 1. **Representation**: multimodal data를 어떻게 표현하고 요약하지?
> > ![image](https://user-images.githubusercontent.com/33504288/124375456-3c26f000-dcdd-11eb-80f4-460370c3f7cc.png)
> > - joint representations: multimodal representation을 concat해서 사용
> > - coordinated representations: 각 modal의 representation에서 필요한 부분들만 뽑아서 사용
> 
> 2. **Alignment**:다른 modal에서 나온 (sub)elenment 간의 직접적인 관계 식별
> > ![image](https://user-images.githubusercontent.com/33504288/124375628-31208f80-dcde-11eb-898e-4da368a0f541.png)
> > 
