## [📔 L1_Course_Introduction]

#### Multi-Modal vs. Multi-Medium
> - **Modality**
> : (이미 일어났거나 경험된) 특정 타입의 정보 or 정보가 저장되는 representation format
> <br>(e.g. Natural language, Visual, Auditory, Haptics/touch, Smell, taste, 
> self-motion, Physiological signals, etc)
> <br> - **Medium**
> : 정보 저장 or 통신을 위한 수단/도구

<br>

#### 4 eras of Multimodal research
> 1. The **"behavioral"** era (1970s ~ late 1980s): McGurk effect 발견...!(하나의 modality만으로는 쪼꼼 힘들겠다~)
> 2. The **"computational"** era (late 1980s ~ 2000): Audio-Visual Speech Recognition, Multimodal/Multisensory interfaces, Multimedia Computing
> 3. The **"interaction"** era (2000 ~ 2010): Modeling Human Multimodal Interaction(e.g. Siri)
> 4. The **"deep learning"** era (2010s ~): Representation learning(New large-scale multimodal datasets, improvement of computing power, high-level visual features, "dimensional" linguistic features)

<br>

#### Core Technical Challenges
> <img src="https://user-images.githubusercontent.com/33504288/124376247-04ba4280-dce1-11eb-8cc4-e4ff031e4d47.png" width="500" height="250">
> 
> _**Representation** 뽑아서 **Alignment**하고 **Translation** or **Fusion**함. (+uni-modal의 training에서 다른 modal 추가로 사용하는 **Co-learning**)_ 
> > 여기서 Translation과 Fusion은 loss function을 뭘 쓰느냐~ 의 차이
> > - translation: 하나의 modal 정보를 다른 modal로 바꾸기(e.g. image captioning)
> > - fusion: 모든 modal을 사용해서 high-level의 무언가로 추론(e.g. emotion recognition, detecting an event in a video)

<br>

#### 🌟 **Representation**: multimodal data를 어떻게 표현하고 요약하지?
 <img src="https://user-images.githubusercontent.com/33504288/124375456-3c26f000-dcdd-11eb-80f4-460370c3f7cc.png" width="500" height="140">

> - **Joint representations**<br>
> : multimodal representation을 concat해서 사용
> - **Coordinated representations**<br>
> : 각 modal의 representation에서 필요한 부분들만 뽑아서 사용

<br>

#### 🌟 **Alignment**:다른 modal에서 나온 (sub)elenment 간의 직접적인 관계 식별
 <img src="https://user-images.githubusercontent.com/33504288/124375628-31208f80-dcde-11eb-898e-4da368a0f541.png" width="550" height="170">
 
> - **Explicit Alignment**<br>
> : modal 간 직접적으로 대응되는 내용 matching(audio:"사과" -> image:사과 사진)
> - **Implicit Alignment**<br>
> : 다른 문제를 더 잘 풀기 위해 내부적으로 잠재된(latent) alignment 사용(Attention 모델이랑 비슷!)
> <img src="https://user-images.githubusercontent.com/33504288/124376074-2c5cdb00-dce0-11eb-8783-e7c9c4c7fa1f.png" width="500" height="150">

<br>

#### 🌟 **Translation**: 다른 modal 형식으로 data를 바꾸자!
<img src="https://user-images.githubusercontent.com/33504288/124376699-37653a80-dce3-11eb-8c1f-8819d73068d3.png" width="550" height="200">

<br>

#### 🌟 **Fusion**: prediction을 위해 두 개 이상의 moidal info를 결합
<img src="https://user-images.githubusercontent.com/33504288/124377360-86f93580-dce6-11eb-95ac-898e362e9c2d.png" width="500" height="200">

> - **Early fusion**<br>
> : multimodal이 low-level일 때 사용! 먼저 fusion하지 않으면 training을 할 수 없을 때...  
> - **Late fusion**<br>
> : fusion 전에 내부적으로 어떠한 processing이 필요할 때 사용
<img src="https://user-images.githubusercontent.com/33504288/124378638-70a2a800-dced-11eb-9643-2051db7b6948.png" width="300" height="140">

> 요즘은 더 다양한 approach들이 많답니당~

<br>

#### 🌟 **Co-Learning**: representation, 예측모델을 포함해서 modal 간의 지식을 전달
 ![image](https://user-images.githubusercontent.com/33504288/124377771-ae510200-dce8-11eb-9982-88ea2d33be7d.png)
> e.g. object detection을 하고 싶은데, training 때 다른 modality를 같이 사용하면 더 도움될 것 같은데?! data가 별로 없는데 다른 modality의 data를 같이 사용하면?!
> - Parallel<br>
> : 서로 다른 modal 간 거의 모든 data에 상응하는 data가 있는 경우
> - Non-Parallel<br>
> : 서로 다른 modal의 data 간 정확히 상응하는 data는 모르지만, 서로 관계가 있을 경우(french sentences corpus & english sentences corpus)

 ![image](https://user-images.githubusercontent.com/33504288/124378095-90849c80-dcea-11eb-819b-c780b96d29dd.png)
> - (traslation의 idea를 이용) verbal modal을 **intermediate representation**(co-learning representation)으로 translate하고, 이를 이용해서 최종 visual을 generate!<br>
> - 이 때, cyclic loss를 이용해서, verbal->visual에서 이용한 정보를 visual->verbal로 regenerate하는 데 사용함. (intermediate representation으로 verbal data의 모든 정보를 가져가겠다!)<br>
> - 한 방향으로만 진행한다면 intermediate representation은 source data(input)보다 target에 훨씬 더 가깝겠지만, cyclic을 추가함으로써 input과도 비슷해짐.

 ![image](https://user-images.githubusercontent.com/33504288/124378421-3389e600-dcec-11eb-8006-f823132d165b.png)
> Test 때는 하나의 modal만 사용(verbal)해서 감정 예측 (놀랍게도 train&test에 multimodal을 사용한 것과 성능이 비슷하다고 함)
